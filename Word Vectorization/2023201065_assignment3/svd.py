# -*- coding: utf-8 -*-
"""svd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1exfmQXH5NxA_hsUxpsLFJfqXpKuNCYkL
"""

import numpy as np
import pandas as pd
import re
import torch
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from scipy.sparse.linalg import svds
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import pickle
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("I am using ",torch.cuda.is_available())
nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenize text
    # Remove stopwords
    # stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens]
    return tokens

def generate_co_occurrence_matrix(corpus, window_size):
    word_index = {}
    index_word = {}
    vocab_index = 0

    # Build vocabulary
    for sentence in corpus:
        for word in sentence:
            if word not in word_index:
                word_index[word] = vocab_index
                index_word[vocab_index] = word
                vocab_index += 1

    # Initialize co-occurrence matrix
    co_occurrence_matrix = np.zeros((len(word_index), len(word_index)))

    # Fill co-occurrence matrix
    for sentence in corpus:
        for i, word in enumerate(sentence):
            if word in word_index:
                for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence))):
                    if i != j and sentence[j] in word_index:
                        co_occurrence_matrix[word_index[word], word_index[sentence[j]]] += 1

    return co_occurrence_matrix.astype(float), word_index

# class WordEmbedding:
#     def __init__(self, window_size=2, vector_size=100):
#         self.window_size = window_size
#         self.vector_size = vector_size
#         self.word_index = {}
#         self.index_word = {}
#         self.co_occurrence_matrix = None
#         self.word_vectors = None

#     def generate_co_occurrence_matrix(self, corpus):
#         # Build vocabulary
#         vocab_index = 0
#         for sentence in corpus:
#             for word in sentence:
#                 if word not in self.word_index:
#                     self.word_index[word] = vocab_index
#                     self.index_word[vocab_index] = word
#                     vocab_index += 1

#         # Initialize co-occurrence matrix
#         self.co_occurrence_matrix = lil_matrix((len(self.word_index), len(self.word_index)))

#         # Fill co-occurrence matrix
#         for sentence in corpus:
#             for i, word in enumerate(sentence):
#                 if word in self.word_index:
#                     for j in range(max(i - self.window_size, 0), min(i + self.window_size + 1, len(sentence))):
#                         if i != j and sentence[j] in self.word_index:
#                             self.co_occurrence_matrix[self.word_index[word], self.word_index[sentence[j]]] += 1
#         co_occurrence_matrix_dense = self.co_occurrence_matrix.toarray()

#         # Print the co-occurrence matrix
#         print(co_occurrence_matrix_dense, self.co_occurrence_matrix.shape)

#     def train_word_vectors(self):
#         # Apply Singular Value Decomposition
#         U, S, Vt = svds(self.co_occurrence_matrix, k=self.vector_size)

#         # Word vectors are stored in the rows of the matrix U
#         self.word_vectors = U

#     def get_word_vector(self, word):
#         if word in self.word_index:
#             return self.word_vectors[self.word_index[word]]
#         else:
#             return None

#     def most_similar_words(self, word, topn=5):
#         if word not in self.word_index:
#             return []

#         word_vector = self.get_word_vector(word)
#         similarities = cosine_similarity([word_vector], self.word_vectors)[0]
#         most_similar_indices = similarities.argsort()[-topn-1:-1][::-1]
#         most_similar_words = [self.index_word[index] for index in most_similar_indices if index != self.word_index[word]]
#         return most_similar_words

# df = pd.read_csv("/content/drive/MyDrive/NLP/ASSIGNMENT 3/train.csv")

# # Extract the Description column
# descriptions = df["Description"].tolist()[:]

# # Preprocessing function

# # Preprocess the descriptions
# preprocessed_descriptions = [preprocess_text(description) for description in descriptions]

# # Train the word embedding model
# embedding_model = WordEmbedding(window_size=2, vector_size=100)
# embedding_model.generate_co_occurrence_matrix(preprocessed_descriptions)
# # embedding_model.train_word_vectors()
# # embedding_model = WordEmbedding(window_size=2, vector_size=3)
# # embedding_model.generate_co_occurrence_matrix(corpus)
# # embedding_model.train_word_vectors()

# # word = "capture"
# # similar_words = embedding_model.most_similar_words(word, topn=2)
# # print(f"Words similar to '{word}': {similar_words}")

df = pd.read_csv("/content/drive/MyDrive/NLP_ASSIGNMENT 3/train.csv", nrows=10000)

# Extract the Description column
descriptions = df["Description"].tolist()
preprocessed_descriptions = [preprocess_text(description) for description in descriptions]
context_window = 6
co_occurrence_matrix, word_index = generate_co_occurrence_matrix(preprocessed_descriptions, context_window)
# Apply Singular Value Decomposition
U, S, Vt = svds(co_occurrence_matrix, k=300)

# Word vectors are stored in the rows of the matrix U
word_vectors = U

embedding_file_name = f'svd-word-vectors_context_window_{context_window}.pt'
    with open(embedding_file_name, 'wb') as f:
        pickle.dump(embeddings, f)

    print(f"Embeddings for context window {context_window} saved successfully.")