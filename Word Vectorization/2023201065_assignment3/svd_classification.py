# -*- coding: utf-8 -*-
"""svd-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DowDlQS3FcUq3-t2Kv_p6VrzU9vU2jvc
"""

# -*- coding: utf-8 -*-
"""svd-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z5hMUEoS4vG3ibd1jWIa0f34MwsUAFjp
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import pickle
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix


df_train = pd.read_csv("/kaggle/input/svd-class/train.csv", nrows=30000)
df_test = pd.read_csv("/kaggle/input/svd-class/test.csv", nrows=30000)
X_train = df_train["Description"].tolist()
y_train = df_train["Class Index"].values
X_test = df_test["Description"].tolist()
y_test = df_test["Class Index"].values

with open('/kaggle/input/contest-win4/svd-word-vectors_context_window_4.pt', 'rb') as f:
    word_vectors_svd = pickle.load(f)
vectorizer = CountVectorizer()
X_train_trans = vectorizer.fit_transform(X_train)
X_test_trans = vectorizer.transform(X_test)

vocab = vectorizer.get_feature_names_out()
def form_word2vec(text_sequences, word_to_idx, word_vectors_from_svd, max_seq_length, input_size):
    batch_size = 1024
    data_sequences = len(text_sequences)
    modified_data = np.zeros((data_sequences, max_seq_length, input_size))
    num_batches = (data_sequences + batch_size - 1) // batch_size

    for batches in range(num_batches):
        start = batches * batch_size
        end = min((batches + 1) * batch_size, data_sequences)

        for i in range(start, end):
            seq = text_sequences[i]
            for j, word in enumerate(seq.split()):
                if j < max_seq_length:
                    if word in word_to_idx:
                        word_index = word_to_idx[word]
                        if word_index < len(word_vectors_from_svd):
                            modified_data[i, j, :] = word_vectors_from_svd[word_index]
                        else:
                            modified_data[i, j, :] = np.zeros(input_size)
                    else:
                        # Handle out-of-vocabulary words
                        modified_data[i, j, :] = np.zeros(input_size)

    return modified_data

word_to_idx = {word: i for i, word in enumerate(vocab)}

max_seq_length = 100
input_size = 300
hidden_size = 128
num_epochs = 10
batch_size = 10
X_train_encoded = form_word2vec(X_train, word_to_idx, word_vectors_svd, max_seq_length,input_size=300)
X_test_encoded = form_word2vec(X_test, word_to_idx, word_vectors_svd, max_seq_length,input_size=300)

# Filter out-of-bounds target labels
default_label = -1
y_train_filtered_Changes = np.where(y_train < np.max(y_train), y_train, default_label)
output_size = len(np.unique(y_train_filtered_Changes))

# Filter training data based on filtered target labels
filtered_indices_on_train = [i for i, label in enumerate(y_train_filtered_Changes) if label != default_label]
X_train_encoded_swapped = X_train_encoded[filtered_indices_on_train]
y_train_filtered_Changes = y_train_filtered_Changes[filtered_indices_on_train]

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train_encoded_swapped, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_encoded, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_filtered_Changes, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Define a default label for unseen classes during testing
default_test_label = default_label
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        output, _ = self.lstm(x)
        output_mean = torch.mean(output, dim=1)
        output = self.fc(output_mean)
        return output

model = LSTMModel(input_size, hidden_size, output_size)

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

train_accuracy = []

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_guess = 0
    total = 0

    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        _, predicted = torch.max(outputs, 1)
        correct_guess += (predicted == y_batch).sum().item()
        total += y_batch.size(0)

    accuracy = correct_guess / total
    train_accuracy.append(accuracy)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}, Training Accuracy: {accuracy}")

plt.plot(range(1, num_epochs + 1), train_accuracy, label='Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Epoch vs Accuracy')
plt.legend()
plt.show()

model.eval()
with torch.no_grad():
    train_outputs = model(X_train_tensor)
    _, train_predicted = torch.max(train_outputs, 1)

train_accuracy = accuracy_score(y_train_filtered_Changes, train_predicted)
train_precision = precision_score(y_train_filtered_Changes, train_predicted, average='weighted')
train_recall = recall_score(y_train_filtered_Changes, train_predicted, average='weighted')
train_f1 = f1_score(y_train_filtered_Changes, train_predicted, average='weighted')
train_confusion = confusion_matrix(y_train_filtered_Changes, train_predicted)

print("\nPerformance Metrics on Train Set:")
print(f"Accuracy: {train_accuracy}")
print(f"Precision: {train_precision}")
print(f"Recall: {train_recall}")
print(f"F1 Score: {train_f1}")
print("Confusion Matrix:")
print(train_confusion)


model.eval()
with torch.no_grad():
    test_outputs = model(X_test_tensor)
    _, predicted = torch.max(test_outputs, 1)
    predicted = np.where(np.isin(predicted, np.unique(y_train)), predicted, default_test_label)

test_accuracy = accuracy_score(y_test, predicted)
test_precision = precision_score(y_test, predicted, average='weighted')
test_recall = recall_score(y_test, predicted, average='weighted')
test_f1 = f1_score(y_test, predicted, average='weighted')
test_confusion = confusion_matrix(y_test, predicted)

print("\nPerformance Metrics on Test Set:")
print(f"Accuracy: {test_accuracy}")
print(f"Precision: {test_precision}")
print(f"Recall: {test_recall}")
print(f"F1 Score: {test_f1}")
print("Confusion Matrix:")
print(test_confusion)

torch.save(model.state_dict(), 'svd-classification-model-ws_3.pt')