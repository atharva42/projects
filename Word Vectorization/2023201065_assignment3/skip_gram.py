# -*- coding: utf-8 -*-
"""skip-gram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UeRBfgF4oPG64Hw13Z2F6yBCxfc6w-Wl

REAL TREASURE LIES BENEATH
"""

import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import random
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
import pickle
nltk.download('punkt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("I am using ",torch.cuda.is_available()).


class SkipGramNSDataset(Dataset):
    def __init__(self, text, word_to_idx, window_size, num_negative_samples):
        self.text = text
        self.word_to_idx = word_to_idx
        self.window_size = window_size
        self.num_negative_samples = num_negative_samples
        self.data = self.generate_data()

    def generate_data(self):
        data = []
        for sentence in self.text:
            for i, target_word in enumerate(sentence):
                if target_word not in self.word_to_idx:
                    continue  # Skip OOV words
                start = max(0, i - self.window_size)
                end = min(len(sentence), i + self.window_size + 1)
                context = [sentence[j] for j in range(start, end) if j != i]
                for context_word in context:
                    if context_word not in self.word_to_idx:
                        continue  # Skip OOV words
                    data.append((self.word_to_idx[target_word], self.word_to_idx[context_word]))
        return data


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        target_word, context_word = self.data[idx]
        negative_samples = self.generate_negative_samples(target_word)
        return target_word, context_word, negative_samples

    def generate_negative_samples(self, target_word):
        negative_samples = []
        while len(negative_samples) < self.num_negative_samples:
            sample = random.randint(0, len(self.word_to_idx) - 1)
            if sample != target_word:
                negative_samples.append(sample)
        return torch.tensor(negative_samples, dtype=torch.long)

# Define the Skip-gram with Negative Sampling model
class SkipGramNS(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramNS, self).__init__()
        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)

    def forward(self, target_word, context_word, negative_samples):
        target_emb = self.target_embeddings(target_word)
        context_emb = self.context_embeddings(context_word)
        neg_emb = self.context_embeddings(negative_samples)

        # Calculate dot products for positive and negative samples
        positive_score = torch.sum(torch.mul(target_emb, context_emb), dim=1)
        negative_score = torch.sum(torch.mul(target_emb.unsqueeze(1), neg_emb), dim=2)

        # Apply sigmoid to convert scores to probabilities
        positive_prob = torch.sigmoid(positive_score)
        negative_prob = torch.sigmoid(-negative_score)

        # Compute loss
        loss = -torch.log(positive_prob).mean() - torch.log(negative_prob).sum(dim=1).mean()
        return loss

# Define text preprocessing function
def preprocess_text(text):
    preprocessed_text = []
    for sentence in text:
        sentence = sentence.lower()
        sentence = ''.join([c for c in sentence if c.isalpha() or c.isspace()])
        tokens = [t for t in word_tokenize(sentence)]
        preprocessed_text.append(tokens)
    return preprocessed_text

# Sample text data
csv_file_path = "/content/drive/MyDrive/NLP/ASSIGNMENT 3/train.csv"

# Load data and preprocess text
data = pd.read_csv(csv_file_path, nrows=100)
text = data['Description']
preprocessed_text = preprocess_text(text)

# Create vocabulary
word_freq = nltk.FreqDist([word for sentence in preprocessed_text for word in sentence])
vocab = sorted(word_freq, key=word_freq.get, reverse=True)

# Sample negative words based on unigram distribution
word_freq = np.array([word_freq[word] for word in vocab])
unigram_dist = word_freq / np.sum(word_freq)
negative_samples = np.random.choice(vocab, size=(len(preprocessed_text), 5), p=unigram_dist)

# Convert words to indices
word_to_idx = {word: i for i, word in enumerate(vocab)}
data_indices = [[word_to_idx[word] for word in sentence if word in vocab] for sentence in preprocessed_text]

# Define dataset and dataloader
vocab_size = len(word_to_idx)
embedding_dim = 300
window_size = 6
num_negative_samples = 5
batch_size = 1024
learning_rate = 0.001
num_epochs = 10

# Create dataset and dataloadewindow_size_values = 6
dataset = SkipGramNSDataset(preprocessed_text, word_to_idx, window_size, num_negative_samples)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize model and optimizer
model = SkipGramNS(vocab_size, embedding_dim)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    total_loss = 0.0
    for target_word, context_word, negative_samples in dataloader:
        optimizer.zero_grad()
        loss = model(target_word, context_word, negative_samples)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')

with open(f'skip-gram-word-vectors_window_{window_size}.pt', 'wb') as f:
        pickle.dump(word_embeddings, f)