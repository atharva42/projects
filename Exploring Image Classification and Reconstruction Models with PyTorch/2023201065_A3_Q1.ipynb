{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define transforms for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = MLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"For epoch {epoch+1}\")\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1562:  # Print every 1562 mini-batches\n",
        "            print(\"Loss: \", running_loss / 2000)\n",
        "            running_loss = 0.0\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'Accuracy : {(correct/total):.2%}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5DmkRyCR-kY",
        "outputId": "579bd1f7-3d4e-402f-df8d-66cf7cb3732f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "For epoch 1\n",
            "Loss:  1.4903423233032227\n",
            "For epoch 2\n",
            "Loss:  1.2621796065568924\n",
            "For epoch 3\n",
            "Loss:  1.1717263000905513\n",
            "For epoch 4\n",
            "Loss:  1.1037519314289093\n",
            "For epoch 5\n",
            "Loss:  1.0480027922391892\n",
            "For epoch 6\n",
            "Loss:  1.001179738789797\n",
            "For epoch 7\n",
            "Loss:  0.9589280534982682\n",
            "For epoch 8\n",
            "Loss:  0.9197740755677223\n",
            "For epoch 9\n",
            "Loss:  0.8835287545919418\n",
            "For epoch 10\n",
            "Loss:  0.8486873773634434\n",
            "Accuracy : 53.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN IMPLEMENTATION"
      ],
      "metadata": {
        "id": "FWNdJTy6dEYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.2154, 0.2024))\n",
        "])\n",
        "\n",
        "train_dataset.transform = transform\n",
        "test_dataset.transform = transform\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "model = CNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if i % 100 == 0:\n",
        "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        #         epoch, i * len(images), len(train_loader.dataset),\n",
        "        #         100. * i / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, criterion, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            test_loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100 * correct / total\n",
        "    print('\\nTest set for: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss, correct, total, accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhJX4Y0RZTUg",
        "outputId": "fc433aa7-ffd1-4fda-d9eb-29a8b51af8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Kw1mVCZrZtdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    train(model, criterion, optimizer, train_loader)\n",
        "    test(model, criterion, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9EtgkVQZu_2",
        "outputId": "7111bf0a-3483-4256-bc84-9f7c869ae362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.301347\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.817148\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.685271\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.574218\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.422637\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.409294\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.610162\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.460630\n",
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 4767/10000 (47.67%)\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.635553\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.397418\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.403135\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.218222\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.257591\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.272875\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.308730\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.145026\n",
            "\n",
            "Test set: Average loss: 0.0204, Accuracy: 5335/10000 (53.35%)\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.263221\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.170089\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.100969\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.000353\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.246739\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.463067\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.158588\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.133971\n",
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 5693/10000 (56.93%)\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.301545\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.932692\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.170908\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.924637\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.125479\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.210050\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.930250\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.824004\n",
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 5947/10000 (59.47%)\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.445668\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.135891\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.041621\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.124298\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.833625\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.004632\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.961020\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.850465\n",
            "\n",
            "Test set: Average loss: 0.0175, Accuracy: 6097/10000 (60.97%)\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.817633\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.889775\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.882471\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.094169\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.125593\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.220209\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.326742\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.083204\n",
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 6140/10000 (61.40%)\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.102763\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.837692\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 1.038080\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 1.371787\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.863757\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.667573\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.966964\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 1.010738\n",
            "\n",
            "Test set: Average loss: 0.0170, Accuracy: 6259/10000 (62.59%)\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.000955\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.782966\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.821636\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.818289\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.923208\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.637251\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 1.092121\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.831748\n",
            "\n",
            "Test set: Average loss: 0.0174, Accuracy: 6215/10000 (62.15%)\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.958939\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.944587\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.807980\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.709292\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 1.048042\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.052853\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 1.097557\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.942385\n",
            "\n",
            "Test set: Average loss: 0.0166, Accuracy: 6367/10000 (63.67%)\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.770218\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 1.029810\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.812039\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.866264\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.908261\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.852380\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.634765\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.730991\n",
            "\n",
            "Test set: Average loss: 0.0171, Accuracy: 6342/10000 (63.42%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG IMPLEMENATION"
      ],
      "metadata": {
        "id": "LVnV3whSdKSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Am I using Gpu? \",torch.cuda.is_available())\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "vgg16.classifier[6] = nn.Linear(num_features, 10)\n",
        "\n",
        "vgg16 = vgg16.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "vgg16.train()\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vgg16(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "vgg16.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = vgg16(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy of VGG is: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "7CWhR-9-DtCq",
        "outputId": "03689006-88cb-4c12-c491-d7abf905e442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am using  True\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:06<00:00, 25163140.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 111MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.788\n",
            "[1,   400] loss: 0.424\n",
            "[1,   600] loss: 0.354\n",
            "[2,   200] loss: 0.236\n",
            "[2,   400] loss: 0.246\n",
            "[2,   600] loss: 0.225\n",
            "[3,   200] loss: 0.166\n",
            "[3,   400] loss: 0.173\n",
            "[3,   600] loss: 0.161\n",
            "[4,   200] loss: 0.109\n",
            "[4,   400] loss: 0.111\n",
            "[4,   600] loss: 0.122\n",
            "[5,   200] loss: 0.074\n",
            "[5,   400] loss: 0.079\n",
            "[5,   600] loss: 0.077\n",
            "[6,   200] loss: 0.048\n",
            "[6,   400] loss: 0.053\n",
            "[6,   600] loss: 0.058\n",
            "[7,   200] loss: 0.043\n",
            "[7,   400] loss: 0.042\n",
            "[7,   600] loss: 0.041\n",
            "[8,   200] loss: 0.030\n",
            "[8,   400] loss: 0.031\n",
            "[8,   600] loss: 0.033\n",
            "[9,   200] loss: 0.020\n",
            "[9,   400] loss: 0.030\n",
            "[9,   600] loss: 0.024\n",
            "[10,   200] loss: 0.015\n",
            "[10,   400] loss: 0.024\n",
            "[10,   600] loss: 0.025\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 93.17 %\n",
            "Accuracy: 0.9317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    MLP:\n",
        "        Test set accuracy: 53.78%\n",
        "        Average loss: The loss starts at around 1.49 in the first epoch and gradually decreases to around 0.85 in the last epoch.\n",
        "\n",
        "    CNN:\n",
        "        Test set accuracy: 63.42%\n",
        "        Average loss: The average loss is reported as 0.0171.\n",
        "\n",
        "    VGG:\n",
        "        Test set accuracy: 93.17%\n",
        "        Average loss: The loss starts relatively high at 0.788 in the first epoch and steadily decreases over training, reaching 0.015 in the last epoch."
      ],
      "metadata": {
        "id": "AOMxlyPEeKUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNNs leverage the spatial structure of images by using convolutional layers to detect patterns and features at different spatial hierarchies. These layers are designed to preserve the spatial relationships within the image, allowing CNNs to capture complex visual information more effectively compared to MLPs. In contrast, MLPs treat images as flattened vectors, ignoring spatial structure, which limits their ability to extract meaningful features from images. This fundamental difference in architecture accounts for CNNs' superior performance in tasks like image classification."
      ],
      "metadata": {
        "id": "7QdywrvfeUr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning with the VGG model improves performance and reduces training time by leveraging pre-learned features from a large dataset (like ImageNet) and adapting them to a smaller target dataset (such as CIFAR-10). This approach saves time by avoiding the need to learn features from scratch and allows the model to quickly adapt to the target dataset's characteristics through fine-tuning. Overall, it offers a more efficient way to build high-performance models for image classification tasks."
      ],
      "metadata": {
        "id": "YV2viMYFeFHF"
      }
    }
  ]
}