# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PM3kV-rklJindxVrv417n1cFK9CGEdEI
"""

!pip install conllu

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from conllu import parse
import torch.nn.functional as F
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix

def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = f.read()
    return parse(data)

train_data = load_data('en_atis-ud-train.conllu')
dev_data = load_data('en_atis-ud-dev.conllu')
test_data = load_data('en_atis-ud-test.conllu')

def evaluate(model, sentences, tags, word_to_idx, tag_to_idx):
    y_true = []
    y_pred = []

    for sent, true_tags in zip(sentences, tags):
        predicted_tags = predict(model, sent, word_to_idx, tag_to_idx)
        y_true.extend(true_tags)
        y_pred.extend(predicted_tags)

    accuracy = accuracy_score(y_true, y_pred)
    recall_micro = recall_score(y_true, y_pred, average='micro')
    recall_macro = recall_score(y_true, y_pred, average='macro')
    f1_micro = f1_score(y_true, y_pred, average='micro')
    f1_macro = f1_score(y_true, y_pred, average='macro')

    confusion_mat = confusion_matrix(y_true, y_pred)

    return accuracy, recall_micro, recall_macro, f1_micro, f1_macro, confusion_mat


def evaluate_model(model, sentences, tags, word_to_idx, tag_to_idx):
    y_true = []
    y_pred = []

    for sent, true_tags in zip(sentences, tags):
        predicted_tags = predict(model, sent, word_to_idx, tag_to_idx)
        y_true.extend(true_tags)
        y_pred.extend(predicted_tags)

    report = classification_report(y_true, y_pred, labels=list(tag_to_idx.keys()), target_names=list(tag_to_idx.keys()), output_dict=True)
    return report

def extract_sentences_and_tags(data):
    sentences = []
    tags = []
    for sentence in data:
        sent = []
        tag = []
        for token in sentence:
            if token['upostag'] != 'PUNCT':
                sent.append(token['form'].lower())
                tag.append(token['upostag'])
        sentences.append(sent)
        tags.append(tag)
    return sentences, tags

train_sentences, train_tags = extract_sentences_and_tags(train_data)
dev_sentences, dev_tags = extract_sentences_and_tags(dev_data)
test_sentences, test_tags = extract_sentences_and_tags(test_data)

def create_vocab_tagset(sentences, tags):
    word_to_idx = {}
    tag_to_idx = {}
    for sentence, tag_seq in zip(sentences, tags):
        for word in sentence:
            if word not in word_to_idx:
                word_to_idx[word] = len(word_to_idx)
        for tag in tag_seq:
            if tag not in tag_to_idx:
                tag_to_idx[tag] = len(tag_to_idx)
    return word_to_idx, tag_to_idx

word_to_idx, tag_to_idx = create_vocab_tagset(train_sentences, train_tags)

embedding_dim = 100
hidden_dim = 128
vocab_size = len(word_to_idx)
tagset_size = len(tag_to_idx)
num_epochs = 30
learning_rate = 0.01

def calculate_accuracy(model, sentences, tags, word_to_idx, tag_to_idx):
    correct = 0
    total = 0
    for sent, true_tags in zip(sentences, tags):
        predicted_tags = predict(model, sent, word_to_idx, tag_to_idx)
        correct += sum(p == t for p, t in zip(predicted_tags, true_tags))
        total += len(true_tags)
    accuracy = correct / total
    return accuracy

def create_vocab_tagset(sentences, tags):
    word_to_idx = {'<unk>': 0}
    tag_to_idx = {}
    for sentence, tag_seq in zip(sentences, tags):
        for word in sentence:
            if word not in word_to_idx:
                word_to_idx[word] = len(word_to_idx)
        for tag in tag_seq:
            if tag not in tag_to_idx:
                tag_to_idx[tag] = len(tag_to_idx)
    return word_to_idx, tag_to_idx
def prepare_sequence(seq, to_idx):
    return torch.tensor([to_idx.get(w, 0) for w in seq], dtype=torch.long)

class LSTMTagger(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, activation='tanh', num_layers=1, bidirectional=False):
        super(LSTMTagger, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.bidirectional = bidirectional

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional)

        if bidirectional:
            self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)
        else:
            self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

        self.activation = getattr(F, activation, None)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = nn.functional.log_softmax(tag_space, dim=1)
        return tag_scores


model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, tagset_size, activation='relu', num_layers=2, bidirectional=True)

# Below is the defination of Loss function and optimizer
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate)

#Here I will be training the model
for epoch in range(num_epochs):
    for sentences, tags in zip(train_sentences, train_tags):
        model.zero_grad()

        sentence_in = prepare_sequence(sentences, word_to_idx)
        targets = prepare_sequence(tags, tag_to_idx)

        # Forward pass
        tag_scores = model(sentence_in)

        # Compute loss
        loss = loss_function(tag_scores, targets)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # if epoch % 1 == 0:
    #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Testing my model on the testset
def predict(model, sentence, word_to_idx, tag_to_idx):
    with torch.no_grad():
        inputs = prepare_sequence(sentence, word_to_idx)
        tag_scores = model(inputs)
        _, predicted_tags = torch.max(tag_scores, 1)
        predicted_tags = [list(tag_to_idx.keys())[list(tag_to_idx.values()).index(idx)] for idx in predicted_tags.tolist()]
        return predicted_tags

# test_sentence = input()
# predicted_tags = predict(model, test_sentence, word_to_idx, tag_to_idx)
# test_accuracy = calculate_accuracy(model, test_sentences, test_tags, word_to_idx, tag_to_idx)

# # Prining the necessary information
# print("Accuracy on Test Dataset:", test_accuracy)
# print("Test Sentence:", test_sentence)
# print("Predicted POS tags:", predicted_tags)
# dev_accuracy, dev_recall_micro, dev_recall_macro, dev_f1_micro, dev_f1_macro, dev_confusion_mat = evaluate(model, dev_sentences, dev_tags, word_to_idx, tag_to_idx)

# # Evaluation on test set
# test_accuracy, test_recall_micro, test_recall_macro, test_f1_micro, test_f1_macro, test_confusion_mat = evaluate(model, test_sentences, test_tags, word_to_idx, tag_to_idx)

# print("Evaluation on Dev Set:")
# print(f"Accuracy: {dev_accuracy}")
# print(f"Recall (Micro): {dev_recall_micro}")
# print(f"Recall (Macro): {dev_recall_macro}")
# print(f"F1 Score (Micro): {dev_f1_micro}")
# print(f"F1 Score (Macro): {dev_f1_macro}")
# print("Confusion Matrix:")
# print(dev_confusion_mat)

# print("*"*13+"-"*13+"*"*13)

# print("\nEvaluation on Test Set:")
# print(f"Accuracy: {test_accuracy}")
# print(f"Recall (Micro): {test_recall_micro}")
# print(f"Recall (Macro): {test_recall_macro}")
# print(f"F1 Score (Micro): {test_f1_micro}")
# print(f"F1 Score (Macro): {test_f1_macro}")
# print("Confusion Matrix:")
# print(test_confusion_mat)

test_sentence = input().split(" ")
predicted_tags = predict(model, test_sentence, word_to_idx, tag_to_idx)
test_accuracy = calculate_accuracy(model, test_sentences, test_tags, word_to_idx, tag_to_idx)

print("Accuracy on Test Dataset:", test_accuracy)
print(test_sentence)
print(predicted_tags)
dev_accuracy, dev_recall_micro, dev_recall_macro, dev_f1_micro, dev_f1_macro, dev_confusion_mat = evaluate(model, dev_sentences, dev_tags, word_to_idx, tag_to_idx)

test_accuracy, test_recall_micro, test_recall_macro, test_f1_micro, test_f1_macro, test_confusion_mat = evaluate(model, test_sentences, test_tags, word_to_idx, tag_to_idx)

print("Evaluation on Dev Set:")
print(f"Accuracy: {dev_accuracy}")
print(f"Recall (Micro): {dev_recall_micro}")
print(f"Recall (Macro): {dev_recall_macro}")
print(f"F1 Score (Micro): {dev_f1_micro}")
print(f"F1 Score (Macro): {dev_f1_macro}")
print("Confusion Matrix:")
print(dev_confusion_mat)

print("\n\nEvaluation on Test Set:")
print(f"Accuracy: {test_accuracy}")
print(f"Recall (Micro): {test_recall_micro}")
print(f"Recall (Macro): {test_recall_macro}")
print(f"F1 Score (Micro): {test_f1_micro}")
print(f"F1 Score (Macro): {test_f1_macro}")
print("Confusion Matrix:")
print(test_confusion_mat)

import matplotlib.pyplot as plt

def train_model(model, num_epochs, train_sentences, train_tags, dev_sentences, dev_tags):
    train_losses = []
    dev_accuracies = []

    for epoch in range(num_epochs):
        epoch_loss = 0.0

        # Training loop
        for sentences, tags in zip(train_sentences, train_tags):
            model.zero_grad()
            sentence_in = prepare_sequence(sentences, word_to_idx)
            targets = prepare_sequence(tags, tag_to_idx)
            tag_scores = model(sentence_in)
            loss = loss_function(tag_scores, targets)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        # Here i am Computing the average training loss for the epoch
        train_loss = epoch_loss / len(train_sentences)
        train_losses.append(train_loss)

        # Here i am Evaluating on development set
        dev_accuracy = calculate_accuracy(model, dev_sentences, dev_tags, word_to_idx, tag_to_idx)
        dev_accuracies.append(dev_accuracy)

        # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Dev Accuracy: {dev_accuracy:.4f}')

    return train_losses, dev_accuracies

# Below is the function to plot epoch vs development set accuracy graph as mentioned
def plot_accuracy_graphs(train_losses_list, dev_accuracies_list, configurations):
    plt.figure(figsize=(10, 6))
    for i, dev_accuracies in enumerate(dev_accuracies_list):
        plt.plot(range(1, len(dev_accuracies)+1), dev_accuracies, label='Configuration {} {}'.format(i+1, configurations[i]['activation']))
    plt.title('Epoch vs Development Set Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.xticks(range(1, len(dev_accuracies)+1))
    plt.grid(True)
    plt.show()

# Experimenting with different hyperparameter configurations
configurations = [
    {'embedding_dim': 50, 'hidden_dim': 50, 'activation': 'tanh', 'num_layers': 1, 'bidirectional': False},
    {'embedding_dim': 100, 'hidden_dim': 100, 'activation': 'relu', 'num_layers': 2, 'bidirectional': True},
    {'embedding_dim': 200, 'hidden_dim': 200, 'activation': 'sigmoid', 'num_layers': 3, 'bidirectional': True}
]

train_losses_list = []
dev_accuracies_list = []

for config in configurations:
    #Part of creating and training model is implemented below
    model = LSTMTagger(config['embedding_dim'], config['hidden_dim'], vocab_size, tagset_size,
                       activation=config['activation'], num_layers=config['num_layers'],
                       bidirectional=config['bidirectional'])
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    train_losses, dev_accuracies = train_model(model, 20, train_sentences, train_tags, dev_sentences, dev_tags)
    print(f"The Dev accuracy for config {i} is {max(dev_accuracies)}")
    train_losses_list.append(train_losses)
    dev_accuracies_list.append(dev_accuracies)

plot_accuracy_graphs(train_losses_list, dev_accuracies_list, configurations)

best_dev_accuracy = max(dev_accuracies_list)
best_config_index = None
for i, accuracy in enumerate(dev_accuracies_list):
    if accuracy == best_dev_accuracy:
        best_config_index = i
        break

if best_config_index is not None:
    best_config = configurations[best_config_index]
print("Index of the best configuration:", best_config_index)
print("Best configuration:", best_config)

best_model = LSTMTagger(best_config['embedding_dim'], best_config['hidden_dim'], vocab_size, tagset_size,
                        activation=best_config['activation'], num_layers=best_config['num_layers'],
                        bidirectional=best_config['bidirectional'])
optimizer = optim.SGD(best_model.parameters(), lr=learning_rate)
train_model(best_model, 20, train_sentences, train_tags, dev_sentences, dev_tags)

test_accuracy = calculate_accuracy(best_model, test_sentences, test_tags, word_to_idx, tag_to_idx)
print(f'Test Accuracy with Best Configuration: {test_accuracy:.2f}')