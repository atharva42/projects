# -*- coding: utf-8 -*-
"""Zomato Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TMfppA7eM-FMK-RY-38n6bL-IuMrNeF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re

reviews = pd.read_csv('https://raw.githubusercontent.com/ammishra08/Recommendation-System/master/Zomato/Restaurant%20reviews.csv')
reviews.head(7)

reviews.shape

reviews.isnull().sum()

reviews.dropna(inplace = True)

reviews_text = reviews[['Review', 'Rating']]
reviews_text

reviews_text['Rating'].unique()

reviews_text['Rating'] = reviews_text['Rating'].replace('Like', 5)

#Typecasting raing in float
reviews_text['Rating'] = reviews_text['Rating'].astype('float')

reviews_text

reviews_text['Rating'].value_counts()

reviews_text['Rating'] = np.where(reviews_text['Rating'] < 4, 0,1)

sns.catplot('Rating', kind='count', data=reviews_text, palette = 'gist_earth_r')

"""**NLP text pre-processor**"""

cleanup_re = re.compile('[^a-z]+')

def clean(sentence):
  sentence = str(sentence)
  sentence = sentence.lower()
  sentence = cleanup_re.sub(' ', sentence).strip()
  return sentence

#apply method is used for applying user declared function to data frames
reviews_text['Review'] = reviews_text['Review'].apply(clean)

reviews_text['Review']

nltk.download('popular')

from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize

def preprocess(sentence):
  sentence = str(sentence)
  word_tokens = word_tokenize(sentence)
  stop_words = set(stopwords.words('english'))
  sentence  = ' '.join(i for i in word_tokens if not i in stop_words)
  return sentence

reviews_text['Review'] = reviews_text['Review'].apply(preprocess)

reviews_text

#Lemmatization
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()

def preprocess_lem (sentence):
    input_str = word_tokenize(sentence)
    lemmatized_op = ' '.join([lemma.lemmatize(w) for w in input_str])
    return lemmatized_op

reviews_text['Review'] = reviews_text['Review'].apply(preprocess_lem)
reviews_text

x = reviews_text['Review']
y = reviews_text['Rating']

"""**LSTM**"""

import keras
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, Embedding, SpatialDropout1D
from keras.layers import LSTM, Dropout, Bidirectional
from keras.callbacks import EarlyStopping

MAX_WORDS = 10000
tokenizer = Tokenizer(num_words = MAX_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)

tokenizer.fit_on_texts(x)

tokenizer.word_index

from keras.preprocessing.sequence import pad_sequences

tokenizer.word_index

x = tokenizer.texts_to_sequences(x)
x = pad_sequences(x, maxlen=50)
print('shape of data tensor is:', x.shape)

x

"""**Split inot Train and test- Cross Validation method**"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)

EMBEDDING_LEN = 100
MAX_LEN = 50

"""**Binary rating classification**"""

model = Sequential()
model.add(Embedding(input_dim=MAX_WORDS, output_dim = EMBEDDING_LEN, input_length = MAX_LEN))
model.add(LSTM(units = 300, recurrent_dropout=0.1))
model.add(Dropout(0.2))
#Last layer = 1(binary classification)
model.add(Dense(1, activation='sigmoid'))
#loss  = 'binary_crossentropy
model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

from keras import callbacks

filepath = '/content/best_model.hdf5'
checkpoint = callbacks.ModelCheckpoint(filepath, moniter = 'val_loss', save_best_only = True, mode = 'min', verbose = 1)

callbacks_list = [checkpoint]

history = model.fit(x_train, y_train,validation_data=(x_test, y_test), epochs = 11, batch_size=64, callbacks = callbacks_list, verbose = 1)

pd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot()

model.load_weights('/content/best_model.hdf5')

model.evaluate(x_test, y_test)

prediction = model.predict(x_test)

np.round(prediction)

"""**Classification Report**

1.   Confusion Matrix


"""

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, np.round(prediction))

sns.heatmap(confusion_matrix(y_test, np.round(prediction)), annot = True, fmt = '0.0f')

